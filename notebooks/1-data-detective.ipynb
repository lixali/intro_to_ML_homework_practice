{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "1-data-detective.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lixali/intro_to_ML_homework_practice/blob/main/notebooks/1-data-detective.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROTbtnV_4FxN"
      },
      "source": [
        "Data detective challenge!\n",
        "=========================\n",
        "\n",
        "*Fraida Fund*"
      ],
      "id": "ROTbtnV_4FxN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Akh9au4FxR"
      },
      "source": [
        "Introduction\n",
        "------------\n",
        "\n",
        "In this notebook, we will consider several machine learning tasks, and\n",
        "candidate data sets for them. We will explore the following questions:\n",
        "\n",
        "-   Do these data sets seem appropriate for the task?\n",
        "-   Are there any important limitations of the datasets, or problems\n",
        "    that need to be addressed before we use them to train a machine\n",
        "    learning model?\n",
        "\n",
        "In fact, each of these datasets has a significant problem that - if not\n",
        "detected early on - would create a “Garbage In, Garbage Out” situation.\n",
        "See if you can identify the problem with each dataset!\n",
        "\n",
        "To get you started, I included some code to show you how to read in the\n",
        "data. You can add additional code and text cells to explore the data.\n",
        "\n",
        "Your work on this challenge won’t be submitted or graded. If you think\n",
        "you found the problem with a dataset, share your findings with the class\n",
        "by posting on Ed! (In your post, show evidence from your exploratory\n",
        "data analysis to support your claims.)"
      ],
      "id": "Y0Akh9au4FxR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0ly0mcV4FxS"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "id": "O0ly0mcV4FxS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHolJLxo4FxU"
      },
      "source": [
        "Taxi tip prediction\n",
        "-------------------"
      ],
      "id": "ZHolJLxo4FxU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbXszHpG4FxU"
      },
      "source": [
        "### Scenario\n",
        "\n",
        "You are developing an app for NYC taxi drivers that will predict what\n",
        "the typical tip would be for a given fare.\n",
        "\n",
        "You consider using data collected by the NYC Taxi and Limousine\n",
        "Commission on taxi trips. These links are for 2019 data (2020 was\n",
        "probably an atypical year, so we won’t use that). Previous years are\n",
        "also available.\n",
        "\n",
        "-   [Data link for yellow (Manhattan) taxi\n",
        "    trips](https://data.cityofnewyork.us/Transportation/2019-Yellow-Taxi-Trip-Data/2upf-qytp)\n",
        "-   [Data link for green (non-Manhattan) taxi\n",
        "    trips](https://data.cityofnewyork.us/Transportation/2019-Green-Taxi-Trip-Data/q5mz-t52e)"
      ],
      "id": "ZbXszHpG4FxU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdDU-7Hf4FxV"
      },
      "source": [
        "### Read in data\n",
        "\n",
        "We’ll start by reading in the 2019 Green Taxi trip data. It’s a large\n",
        "file and takes a long time to download, so we may interrupt the download\n",
        "in middle (using the Runtime menu in Colab) and just work with the\n",
        "partial data.\n",
        "\n",
        "In the next couple of cells, `wget` and `wc` are not Python code -\n",
        "they’re Linux commands. We can run some basic Linux commands inside our\n",
        "Colab runtime, and it’s often helpful to do so. For example, we may use\n",
        "Linux commands to install extra software libraries that are not\n",
        "pre-installed in our runtime, clone a source code repository from\n",
        "Github, or download data from the Internet."
      ],
      "id": "CdDU-7Hf4FxV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqsRUWIo4FxW"
      },
      "source": [
        "!wget \"https://data.cityofnewyork.us/api/views/q5mz-t52e/rows.csv?accessType=DOWNLOAD\" -O 2019-Green-Taxi-Trip-Data.csv"
      ],
      "id": "wqsRUWIo4FxW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe-5yJU84FxX"
      },
      "source": [
        "Is the cell above taking a long time to run? That’s because this data\n",
        "set is very large, and the server from which it is retrieved is not very\n",
        "fast. Since we don’t need to explore the whole dataset, necessarily, we\n",
        "can interrupt the partial download by using the Runtime \\> Interrupt\n",
        "Execution menu option.\n",
        "\n",
        "Then, we can read in just 10,000 rows of data."
      ],
      "id": "oe-5yJU84FxX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCYpm4L64FxY"
      },
      "source": [
        "df_taxi = pd.read_csv('2019-Green-Taxi-Trip-Data.csv', nrows=10000)   \n",
        "df_taxi.head()"
      ],
      "id": "SCYpm4L64FxY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGrIIzJt4FxZ"
      },
      "source": [
        "Use additional cells as needed to explore this data. Answer the\n",
        "following questions:\n",
        "\n",
        "-   How is the data collected? Is it automatic, or is there human\n",
        "    involvement?\n",
        "-   What variable should be the *target variable* for this machine\n",
        "    learning problem?\n",
        "-   What variable(s) could potentially be used as *features* to train\n",
        "    the model?\n",
        "-   What are our assumptions about the features and the target variable,\n",
        "    and the relationships between these? (For example: in NYC, what is a\n",
        "    conventional tip amount, as a percent of the total fare? If you are\n",
        "    not from NYC, you can find information about this online!) Are any\n",
        "    of these assumptions violated in this data?\n",
        "-   Are there variables that should *not* be used as features to train\n",
        "    the model, because of potential for data leakage?\n",
        "-   Are there any serious data problems that we need to correct before\n",
        "    using the data for this purpose? Explain."
      ],
      "id": "XGrIIzJt4FxZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgcEhMrk4Fxa"
      },
      "source": [
        "Highway traffic prediction\n",
        "--------------------------"
      ],
      "id": "fgcEhMrk4Fxa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALZWAwC84Fxa"
      },
      "source": [
        "### Scenario\n",
        "\n",
        "You are working for the state of New York to develop a traffic\n",
        "prediction model for the NYS Thruway. The following Thruway data is\n",
        "available: Number and types of vehicles that entered from each entry\n",
        "point on the Thruway, along with their exit points, at 15 minute\n",
        "intervals.\n",
        "\n",
        "The link points to the most recent week’s worth of available data, but\n",
        "this data is available through 2014. [Link to NYS Thruway\n",
        "data](https://data.ny.gov/Transportation/NYS-Thruway-Origin-and-Destination-Points-for-All-/4dbf-24u2)"
      ],
      "id": "ALZWAwC84Fxa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkBsprlQ4Fxa"
      },
      "source": [
        "### Read in data"
      ],
      "id": "CkBsprlQ4Fxa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5fbVKvz4Fxb"
      },
      "source": [
        "url = 'https://data.ny.gov/api/views/4dbf-24u2/rows.csv?accessType=DOWNLOAD&sorting=true'\n",
        "df_thruway = pd.read_csv(url)\n",
        "df_thruway.head()"
      ],
      "id": "t5fbVKvz4Fxb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3lkwXDL4Fxb"
      },
      "source": [
        "Use additional cells as needed to explore this data. Answer the\n",
        "following questions:\n",
        "\n",
        "-   How is the data collected? Is it automatic, or is there human\n",
        "    involvement?\n",
        "-   What variable should be the *target variable* for this machine\n",
        "    learning problem?\n",
        "-   What variable(s) could potentially be used as *features* to train\n",
        "    the model?\n",
        "-   What are our assumptions about the features and the target variable,\n",
        "    and the relationships between these? (For example: what times of day\n",
        "    should be busy? What times of day will be less busy? What stretches\n",
        "    of the Thruway might be especially congested - look at Google Maps?)\n",
        "-   Are there variables that should *not* be used as features to train\n",
        "    the model, because of potential for data leakage?\n",
        "-   Are there any serious data problems that we need to correct before\n",
        "    using the data for this purpose? Explain."
      ],
      "id": "a3lkwXDL4Fxb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfSuncPy4Fxc"
      },
      "source": [
        "Satirical headline classification\n",
        "---------------------------------"
      ],
      "id": "XfSuncPy4Fxc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxsqJWcb4Fxc"
      },
      "source": [
        "### Scenario\n",
        "\n",
        "You are hired by a major social media platform to develop a machine\n",
        "learning model that will be used to clearly mark *satirical news\n",
        "articles* when they are shared on social media.\n",
        "\n",
        "You consider using this dataset of 9,000 headlines from [The\n",
        "Onion](https://www.theonion.com/) and 15,000 headlines from [Not The\n",
        "Onion on Reddit](https://www.reddit.com/r/nottheonion/). [Link to\n",
        "OnionOrNot data](https://github.com/lukefeilberg/onion)\n",
        "\n",
        "([This\n",
        "notebook](https://github.com/lukefeilberg/onion/blob/master/Onion.ipynb)\n",
        "shows how the data was compiled and processed.)"
      ],
      "id": "dxsqJWcb4Fxc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc6BSEt74Fxc"
      },
      "source": [
        "### Read in data\n",
        "\n",
        "This time, we’ll retrieve the data from Github."
      ],
      "id": "cc6BSEt74Fxc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdYYE1c14Fxd"
      },
      "source": [
        "!git clone https://github.com/lukefeilberg/onion.git"
      ],
      "id": "MdYYE1c14Fxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeRvXc5O4Fxd"
      },
      "source": [
        "df_headline = pd.read_csv(\"onion/OnionOrNot.csv\")\n",
        "df_headline.head()"
      ],
      "id": "oeRvXc5O4Fxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JBHlauA4Fxd"
      },
      "source": [
        "Use additional cells as needed to explore this data. Answer the\n",
        "following questions:\n",
        "\n",
        "-   How is the data collected? Is it automatic, or is there human\n",
        "    involvement?\n",
        "-   What variable should be the *target variable* for this machine\n",
        "    learning problem?\n",
        "-   What variable(s) could potentially be used as *features* to train\n",
        "    the model?\n",
        "-   What are our assumptions about the data?\n",
        "-   Are there variables that should *not* be used as features to train\n",
        "    the model, because of potential for data leakage?\n",
        "-   Are there any serious data problems that we need to correct before\n",
        "    using the data for this purpose? Explain."
      ],
      "id": "9JBHlauA4Fxd"
    }
  ]
}